{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "pa7e7zOZxrsn",
    "outputId": "c77b5654-66f5-4f53-8068-cd3f59483f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 5.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 8.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 81kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 122kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 143kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 153kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 163kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 174kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 184kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 204kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 215kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 235kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 245kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 256kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 266kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 276kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 286kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 296kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 307kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 317kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 327kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 337kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 348kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 358kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 368kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 378kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 389kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 399kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 409kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 419kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 430kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 440kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 450kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 460kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 471kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 481kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 491kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 501kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 512kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 522kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 532kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 542kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 552kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 563kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 573kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 583kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 593kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 604kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 614kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 624kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 634kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 645kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 655kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 665kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 675kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 686kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 696kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 706kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 716kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 727kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 737kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 747kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 757kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 768kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 778kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 788kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 798kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 808kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 819kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 829kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 839kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 849kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 860kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 870kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 880kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 890kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 901kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 911kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 921kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 931kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 942kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 952kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 962kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 972kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 983kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 993kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.0MB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.0MB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.0MB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.0MB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0MB 9.3MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.85\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "!pip install sentencepiece\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P54fvfBwyAt_",
    "outputId": "c242ccee-5281-42d1-9fca-15fd6ccd20bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.60', '1.4.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai, torch\n",
    "fastai.__version__ , torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWl6kOp-yC_r"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('drive/My Drive/nlp-telugu/telugu_wikipedia_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9uky3HYLyKoT",
    "outputId": "c487d052-c8a7-47be-c718-abde76d04b76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90000, 2), Index(['title', 'text'], dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2Plc5oruyNFr",
    "outputId": "2ec54a85-6cc2-4ae5-b86c-03180532a101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83953\n"
     ]
    }
   ],
   "source": [
    "texts = (df['title'] + ' ' + df['text']).tolist()\n",
    "texts = [t for t in df['text'] if len(t.strip())>1]\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qx-Cp663yPqB"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(texts, columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LIA4xXGLySEi",
    "outputId": "a2c69b81-54ff-4c8b-d35a-1a13515e14f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df, texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5sjRDA6KyUNg",
    "outputId": "9dea7aa5-962a-4882-b4cb-cc7763d838dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67162, 1), (16791, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df2, test_size=0.2)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpXvVbpLyWwE"
   },
   "outputs": [],
   "source": [
    "from inltk.tokenizer import TeluguTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVv9GME9ynNm"
   },
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')\n",
    "itos = [sp.IdToPiece(int(i)) for i in range(25000)] # 25,000 is the vocab_size selected in sm tokenizer\n",
    "telugu_vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ot7t0U0lypom"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tok_func=TeluguTokenizer, lang='te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "FFAG-pLTyrzH",
    "outputId": "9ca3733b-e4df-4cdd-c20e-a904e0584d1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUdav3a8ytmz"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "UptKjr4HyvNl",
    "outputId": "6641701a-2b29-4c43-b6e2-2a7595817b9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_df('/content/', train_df=train_df, valid_df=test_df,\n",
    "                                  text_cols=[\"text\"], bs=96,\n",
    "                                  tokenizer=tokenizer, vocab=telugu_vocab) # default bs=64\n",
    "data_lm.save('telugu_lm.data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cHj5oqgyzO_",
    "outputId": "768f217c-1a72-4bc4-a09b-d84f2c9f97d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "EMc3Qyr1y2Uo",
    "outputId": "130d1ff7-fe61-42a9-968a-31cdba51112e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁ఇది ▁మండల ▁కేంద్రమైన ▁రావి క మతం ▁నుండి ▁10 ▁కి . ▁మీ . ▁దూరం ▁లోను , ▁సమీప ▁పట్టణమైన ▁అనకాపల్లి ▁నుండి ▁45 ▁కి . ▁మీ . ▁దూరంలోనూ ▁ఉంది . ▁2011 ▁భారత ▁జనగణన ▁గణాంకాల ▁ప్రకారం ▁ఈ ▁గ్రామం ▁39 52 ▁ఇళ్లతో , ▁146 69 ▁జనాభాతో ▁10 52 ▁హెక్టార్లలో ▁విస్తరించి ▁ఉంది . ▁గ్రామంలో ▁మగవారి ▁సంఖ్య ▁70 66 , ▁ఆడవారి ▁సంఖ్య ▁76 03 . ▁షెడ్యూల్డ్ ▁కులాల ▁సంఖ్య ▁1893 ▁కాగా ▁షెడ్యూల్డ్ ▁తెగల ▁సంఖ్య ▁35 . ▁గ్రామం ▁యొక్క</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁కంటే ▁ఎక్కువ ▁దూరంలో ▁ఉన్నాయి . ▁గ్రామంలో ▁కుళాయిల ▁ద్వారా ▁రక్షిత ▁మంచినీటి ▁సరఫరా ▁జరుగుతోంది . ▁బావుల ▁నీరు ▁కూడా ▁అందుబాటులో ▁ఉంది . ▁గ్రామంలో ▁ఏడాది ▁పొడుగునా ▁చేతిపంపుల ▁ద్వారా ▁నీరు ▁అందుతుంది . ▁బోరుబావుల ▁ద్వారా ▁కూడా ▁ఏడాది ▁పొడుగునా ▁నీరు ▁అందుతుంది . ▁చెరువు ▁ద్వారా ▁గ్రామానికి ▁తాగునీరు ▁లభిస్తుంది . ▁గ్రామంలో ▁మురుగునీటి ▁పారుదల ▁వ్యవస్థ ▁లేదు . ▁మురుగునీటి ని ▁నేరుగా ▁జలవనరుల్లోకి ▁వదులుతున్న ారు . ▁గ్రామంలో ▁సంపూర్ణ ▁పారిశుధ్య ▁పథకం ▁అమలవుతోంది . ▁సామాజిక ▁మరుగుదొడ్డి ▁సౌకర్యం ▁లేదు . ▁ఇంటింటిక ీ ▁తిరిగి ▁వ్యర్థాలను ▁సేకరించే</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>, ▁లక్నో , ▁ దారు ల్ ▁ఇస్లాం , ▁సుల్తాన్ ▁పూర్ , ▁తుగ్లక్ ▁పూర్ , ▁దౌ ల త ాబాదు , ▁ముల్ క్ - ఎ - తి ల ంగ్ ▁( తెలంగాణ ా ) ▁లలో ▁ముద్రించ ేవాడు . ▁ఇ ంతవరకూ ▁30 ▁రకాల ▁బిల్ లన్ ▁నాణేల ▁గూర్చి ▁తెలిసింది . ▁తుగ్లక్ ▁కాలం ▁నాటి ▁నాణెం ▁బలవంతంగా ▁ప్రవేశ ▁పెట్ట బడిన ▁నాణెం ▁తుగ్లక్ ▁అనీ , ▁పిచ్చి ▁తుగ్లక్ ▁అనీ , ▁తెలుగు ▁సినిమాలలో ▁సైతం , ▁ఇతడి ▁పేరు ▁ఒక ▁తరం లో</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁అతనిని ▁నా ▁గురువుగా ▁ , ▁' పథ ▁నిర్దేశ కు ని ' ▁గా ▁భావ ిస్తాను \" ▁అని ▁ఆయన ▁అన్నాడు . ▁' తిరు ప్పా వై ' ▁పై ▁తెలుగులో ▁శ్రీ బా ష్య ం ▁చేసిన ▁ఉపన్యాసాల ▁ నోట్స్ ▁తీసుకున్నాడు . ▁తరువాత ▁అతను ▁' తిరు ప్పా వై ' ▁ను ▁ఆంగ్లంలో కి ▁అనువదించాడు . ▁దీనిని ▁ఆంగ్ల ▁అనువాద ాన్ని ▁తిరుమల ▁తిరుపతి ▁దేవస్థాన ాలు ▁ప్రచురించ ాయి . ▁శ్రీ బా ష్య ం ▁' ▁వాల్మీకి ▁రామాయణం ▁' ▁గురించి ▁రాసిన ▁తెలుగు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁జరిగిన ▁చాలా ▁నెలల ▁తరువాత ▁భారత్ ▁తన విగా ▁భావిస్తున్న ▁అ క్సా య్ ▁చి న్ ▁వంటి ▁అనేక ▁ప్రాంతాలను ▁చూప ిస్తూ ▁భారతదేశ ▁మ్యా పు లను ▁ప్రచురించ వలసినదిగా ▁నెహ్రూ ▁ఆదేశించాడు .[30] ▁ఈ ▁కొత్త ▁మ్యా పు ల్లో &lt;unk&gt; ▁n e f a ▁ప్రాంతంలోని ▁శిఖరాల ను ▁సరిహద్దు గా ▁సూచించారు . ▁ఈ ▁సరిహద్దు ▁మెక్ ▁మహా న్ ▁రేఖ కు ▁ఉత్తర ంగా ▁ఉన్నప్పటికీ ▁అలాగే ▁చూపించారు .[8] ▁1959 ▁లో ▁టిబెట్ ▁తిరుగుబాటు ▁విఫలమవ డం , ▁14 ▁వ ▁ద లై లా మా</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fFAAcH4vy389",
    "outputId": "14509d25-c37a-4992-8670-258661b2c31e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBeQXw1ty54U"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mQNvNBofzzrr",
    "outputId": "8b4bcd97-9bb7-46bc-c045-1052ef282283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODjUQyJ6z3XZ"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "eWyhnX5Fz7BX",
    "outputId": "dab93e2d-7d51-431d-ba0b-832c0b146562"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEGCAYAAABCa2PoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIRCihiIhIRCihiIhIRCihiIhIRPx/SUh1z+/W6B0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GyK6tD410VNQ",
    "outputId": "74774b4d-931b-4b57-e513-51002b5c9d92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(25000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=25000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XvV49BB0wPs"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "FqooHsD90XfJ",
    "outputId": "3311f37f-0b67-4b4e-a529-7c6e02a9f80d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.418655</td>\n",
       "      <td>3.382510</td>\n",
       "      <td>0.479361</td>\n",
       "      <td>2:10:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))\n",
    "learn.save('first', with_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sFHO0bC0dcA"
   },
   "outputs": [],
   "source": [
    "TEXT = \"ఇది మండల కేంద్రమైన రంపచోడవరం నుండి\"\n",
    "N_WORDS = 20\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "pKDoYnNAT8HZ",
    "outputId": "7d29f912-e06e-4329-eca9-b8e042d35a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ఇది మండల కేంద్రమైన రంపచోడవరం నుండి ▁25 ▁కి . ▁మీ . ▁దూరం ▁లోను , ▁సమీప ▁పట్టణమైన ▁తుని ▁నుండి ▁18 ▁కి . ▁మీ . ▁దూరంలోనూ ▁ఉంది .\n",
      "ఇది మండల కేంద్రమైన రంపచోడవరం నుండి ▁11 ▁కి . ▁మీ . ▁దూరం ▁లోను , ▁సమీప ▁పట్టణమైన ▁విశాఖపట్నం ▁నుండి ▁178 ▁కి . ▁మీ . ▁దూరంలోనూ ▁ఉంది .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.9) for _ in range(N_SENTENCES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1bydPxVHT-BF",
    "outputId": "d9c2c04d-0cc2-4e5a-958c-1375691fe86e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.44458434563783"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.38251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Pacb28zUFHd"
   },
   "outputs": [],
   "source": [
    "defaults.device = torch.device('cpu')\n",
    "learn.model.eval()\n",
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIq57UW4Ufdf"
   },
   "outputs": [],
   "source": [
    "!cp -r 'export.pkl' 'drive/My Drive/nlp-telugu/transformer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtTf_oZvUnjq"
   },
   "outputs": [],
   "source": [
    "defaults.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "qfjMBmvvUuTq",
    "outputId": "546d5611-1235-4c9e-f0eb-b1d221ded478"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = load_learner('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EB8eMrxfUx8Q",
    "outputId": "0a379324-4bbf-447a-cb9b-7f5b30df6dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 410])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = get_model(learn.model)[0]\n",
    "encoder.state_dict()['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TCBPIlXtU6G2",
    "outputId": "e464e668-86c5-4abe-90c2-80f703bbff1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410,)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = encoder.state_dict()['encoder.weight']\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZXEvwYPKb3VS",
    "outputId": "0ae18c96-b476-4618-f349-a31b2c5ac540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 410)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(embeddings)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJ8StDUKb-8o"
   },
   "outputs": [],
   "source": [
    "df.to_csv('embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "-I_R9RFtcAVp",
    "outputId": "930be555-3ea1-443c-9ba0-0c08573bf848"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.168456</td>\n",
       "      <td>0.250936</td>\n",
       "      <td>0.103543</td>\n",
       "      <td>-0.172454</td>\n",
       "      <td>-0.041535</td>\n",
       "      <td>-0.045538</td>\n",
       "      <td>0.061835</td>\n",
       "      <td>0.232078</td>\n",
       "      <td>-0.176761</td>\n",
       "      <td>0.153493</td>\n",
       "      <td>-0.011900</td>\n",
       "      <td>-0.187309</td>\n",
       "      <td>-0.233887</td>\n",
       "      <td>-0.147983</td>\n",
       "      <td>0.106419</td>\n",
       "      <td>0.175018</td>\n",
       "      <td>-0.049819</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>-0.034908</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.110372</td>\n",
       "      <td>0.146387</td>\n",
       "      <td>-0.130649</td>\n",
       "      <td>-0.140149</td>\n",
       "      <td>-0.301429</td>\n",
       "      <td>-0.011796</td>\n",
       "      <td>-0.036150</td>\n",
       "      <td>0.098545</td>\n",
       "      <td>-0.143862</td>\n",
       "      <td>-0.007573</td>\n",
       "      <td>-0.024107</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>-0.197268</td>\n",
       "      <td>-0.038095</td>\n",
       "      <td>0.152785</td>\n",
       "      <td>0.290618</td>\n",
       "      <td>0.189962</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>-0.065284</td>\n",
       "      <td>0.271778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117204</td>\n",
       "      <td>-0.101579</td>\n",
       "      <td>-0.035541</td>\n",
       "      <td>-0.161840</td>\n",
       "      <td>0.170878</td>\n",
       "      <td>-0.186538</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>0.220164</td>\n",
       "      <td>-0.063298</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>0.113246</td>\n",
       "      <td>-0.269617</td>\n",
       "      <td>-0.105775</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.209866</td>\n",
       "      <td>-0.133459</td>\n",
       "      <td>0.073281</td>\n",
       "      <td>-0.031263</td>\n",
       "      <td>0.415724</td>\n",
       "      <td>0.151674</td>\n",
       "      <td>-0.171354</td>\n",
       "      <td>0.145311</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-0.101140</td>\n",
       "      <td>-0.064890</td>\n",
       "      <td>0.122351</td>\n",
       "      <td>-0.207216</td>\n",
       "      <td>-0.157048</td>\n",
       "      <td>0.308678</td>\n",
       "      <td>-0.134374</td>\n",
       "      <td>-0.121189</td>\n",
       "      <td>0.199974</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>-0.021250</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>-0.291452</td>\n",
       "      <td>0.143585</td>\n",
       "      <td>0.061026</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.155917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.220249</td>\n",
       "      <td>-0.188598</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0.112449</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.101148</td>\n",
       "      <td>-0.033054</td>\n",
       "      <td>-0.144005</td>\n",
       "      <td>0.423079</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.057473</td>\n",
       "      <td>-0.179813</td>\n",
       "      <td>0.037655</td>\n",
       "      <td>-0.185398</td>\n",
       "      <td>0.250641</td>\n",
       "      <td>0.082603</td>\n",
       "      <td>0.149942</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.287772</td>\n",
       "      <td>0.145918</td>\n",
       "      <td>-0.398235</td>\n",
       "      <td>-0.076679</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.432985</td>\n",
       "      <td>0.510667</td>\n",
       "      <td>-0.146145</td>\n",
       "      <td>-0.098704</td>\n",
       "      <td>0.074941</td>\n",
       "      <td>0.277632</td>\n",
       "      <td>0.215441</td>\n",
       "      <td>0.297296</td>\n",
       "      <td>-0.539252</td>\n",
       "      <td>0.032001</td>\n",
       "      <td>-0.156901</td>\n",
       "      <td>-0.296157</td>\n",
       "      <td>-0.043996</td>\n",
       "      <td>-0.093337</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>-0.083677</td>\n",
       "      <td>-0.132683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282637</td>\n",
       "      <td>-0.166021</td>\n",
       "      <td>-0.158601</td>\n",
       "      <td>0.300466</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.216768</td>\n",
       "      <td>0.205630</td>\n",
       "      <td>-0.148975</td>\n",
       "      <td>-0.038599</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.289732</td>\n",
       "      <td>0.540004</td>\n",
       "      <td>0.057157</td>\n",
       "      <td>0.116406</td>\n",
       "      <td>-0.278459</td>\n",
       "      <td>0.028926</td>\n",
       "      <td>0.160498</td>\n",
       "      <td>0.369616</td>\n",
       "      <td>-0.340823</td>\n",
       "      <td>-0.306371</td>\n",
       "      <td>-0.129633</td>\n",
       "      <td>0.073938</td>\n",
       "      <td>0.078633</td>\n",
       "      <td>-0.234326</td>\n",
       "      <td>-0.124070</td>\n",
       "      <td>-0.263269</td>\n",
       "      <td>0.102418</td>\n",
       "      <td>0.114218</td>\n",
       "      <td>-0.386652</td>\n",
       "      <td>0.219183</td>\n",
       "      <td>0.151405</td>\n",
       "      <td>-0.082154</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.136987</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>0.396601</td>\n",
       "      <td>-0.054791</td>\n",
       "      <td>0.152203</td>\n",
       "      <td>-0.138082</td>\n",
       "      <td>-0.218087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.221206</td>\n",
       "      <td>-0.199571</td>\n",
       "      <td>-0.370084</td>\n",
       "      <td>0.126621</td>\n",
       "      <td>0.025762</td>\n",
       "      <td>0.102211</td>\n",
       "      <td>-0.025885</td>\n",
       "      <td>-0.145297</td>\n",
       "      <td>0.439236</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.062535</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>0.230115</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.104293</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.268263</td>\n",
       "      <td>0.128675</td>\n",
       "      <td>-0.382109</td>\n",
       "      <td>-0.101414</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.499839</td>\n",
       "      <td>-0.146140</td>\n",
       "      <td>-0.121144</td>\n",
       "      <td>0.075234</td>\n",
       "      <td>0.291923</td>\n",
       "      <td>0.218704</td>\n",
       "      <td>0.311183</td>\n",
       "      <td>-0.528095</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>-0.182094</td>\n",
       "      <td>-0.291808</td>\n",
       "      <td>-0.052686</td>\n",
       "      <td>-0.086928</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.067001</td>\n",
       "      <td>-0.154394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278401</td>\n",
       "      <td>-0.165919</td>\n",
       "      <td>-0.141969</td>\n",
       "      <td>0.323553</td>\n",
       "      <td>0.120676</td>\n",
       "      <td>0.203688</td>\n",
       "      <td>0.198097</td>\n",
       "      <td>-0.149195</td>\n",
       "      <td>-0.029635</td>\n",
       "      <td>0.115855</td>\n",
       "      <td>0.310436</td>\n",
       "      <td>0.559299</td>\n",
       "      <td>0.054549</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>-0.262636</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.157656</td>\n",
       "      <td>0.375482</td>\n",
       "      <td>-0.344028</td>\n",
       "      <td>-0.294314</td>\n",
       "      <td>-0.096098</td>\n",
       "      <td>0.078326</td>\n",
       "      <td>0.067719</td>\n",
       "      <td>-0.236995</td>\n",
       "      <td>-0.133526</td>\n",
       "      <td>-0.257952</td>\n",
       "      <td>0.152134</td>\n",
       "      <td>0.113968</td>\n",
       "      <td>-0.371074</td>\n",
       "      <td>0.230251</td>\n",
       "      <td>0.154383</td>\n",
       "      <td>-0.068707</td>\n",
       "      <td>0.040037</td>\n",
       "      <td>0.141809</td>\n",
       "      <td>0.047365</td>\n",
       "      <td>0.377341</td>\n",
       "      <td>-0.068729</td>\n",
       "      <td>0.150654</td>\n",
       "      <td>-0.132315</td>\n",
       "      <td>-0.233182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242197</td>\n",
       "      <td>0.227901</td>\n",
       "      <td>0.124662</td>\n",
       "      <td>-0.277270</td>\n",
       "      <td>-0.232432</td>\n",
       "      <td>-0.123582</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>-0.071580</td>\n",
       "      <td>0.291700</td>\n",
       "      <td>0.341650</td>\n",
       "      <td>-0.336078</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>-0.267962</td>\n",
       "      <td>-0.150138</td>\n",
       "      <td>0.498996</td>\n",
       "      <td>-0.204145</td>\n",
       "      <td>0.190308</td>\n",
       "      <td>-0.114767</td>\n",
       "      <td>-0.302684</td>\n",
       "      <td>0.104871</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>0.266462</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>-0.279269</td>\n",
       "      <td>0.307306</td>\n",
       "      <td>0.234660</td>\n",
       "      <td>-0.319518</td>\n",
       "      <td>-0.252226</td>\n",
       "      <td>0.202733</td>\n",
       "      <td>-0.314149</td>\n",
       "      <td>0.250380</td>\n",
       "      <td>-0.181458</td>\n",
       "      <td>-0.226488</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.208720</td>\n",
       "      <td>0.121967</td>\n",
       "      <td>0.165322</td>\n",
       "      <td>-0.166402</td>\n",
       "      <td>0.306722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330487</td>\n",
       "      <td>0.199902</td>\n",
       "      <td>0.206052</td>\n",
       "      <td>0.421675</td>\n",
       "      <td>0.217915</td>\n",
       "      <td>0.187960</td>\n",
       "      <td>-0.222863</td>\n",
       "      <td>0.280560</td>\n",
       "      <td>0.078392</td>\n",
       "      <td>-0.406938</td>\n",
       "      <td>0.242978</td>\n",
       "      <td>-0.308894</td>\n",
       "      <td>-0.220781</td>\n",
       "      <td>0.044165</td>\n",
       "      <td>0.208608</td>\n",
       "      <td>0.309743</td>\n",
       "      <td>-0.383192</td>\n",
       "      <td>0.097210</td>\n",
       "      <td>0.253515</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>0.167909</td>\n",
       "      <td>0.162783</td>\n",
       "      <td>-0.200553</td>\n",
       "      <td>0.145214</td>\n",
       "      <td>-0.290715</td>\n",
       "      <td>0.222507</td>\n",
       "      <td>0.546484</td>\n",
       "      <td>-0.217568</td>\n",
       "      <td>0.209692</td>\n",
       "      <td>-0.378208</td>\n",
       "      <td>-0.124360</td>\n",
       "      <td>0.256641</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>-0.099903</td>\n",
       "      <td>-0.221089</td>\n",
       "      <td>0.314803</td>\n",
       "      <td>-0.174330</td>\n",
       "      <td>0.278329</td>\n",
       "      <td>0.182786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.350860</td>\n",
       "      <td>0.209580</td>\n",
       "      <td>0.249544</td>\n",
       "      <td>-0.309201</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>-0.198060</td>\n",
       "      <td>-0.070070</td>\n",
       "      <td>0.255033</td>\n",
       "      <td>-0.190754</td>\n",
       "      <td>0.138165</td>\n",
       "      <td>-0.190626</td>\n",
       "      <td>-0.130491</td>\n",
       "      <td>-0.185311</td>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.013398</td>\n",
       "      <td>-0.312968</td>\n",
       "      <td>-0.268597</td>\n",
       "      <td>0.204573</td>\n",
       "      <td>-0.187261</td>\n",
       "      <td>-0.336301</td>\n",
       "      <td>-0.085469</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>-0.289729</td>\n",
       "      <td>-0.228696</td>\n",
       "      <td>-0.182647</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>0.154409</td>\n",
       "      <td>-0.208595</td>\n",
       "      <td>-0.266289</td>\n",
       "      <td>-0.139497</td>\n",
       "      <td>-0.304062</td>\n",
       "      <td>0.265459</td>\n",
       "      <td>-0.235694</td>\n",
       "      <td>-0.053357</td>\n",
       "      <td>0.167990</td>\n",
       "      <td>0.215649</td>\n",
       "      <td>-0.359658</td>\n",
       "      <td>0.238233</td>\n",
       "      <td>-0.060195</td>\n",
       "      <td>0.198973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246661</td>\n",
       "      <td>0.242423</td>\n",
       "      <td>0.048206</td>\n",
       "      <td>-0.118178</td>\n",
       "      <td>-0.193151</td>\n",
       "      <td>-0.118218</td>\n",
       "      <td>-0.104784</td>\n",
       "      <td>0.267716</td>\n",
       "      <td>0.139456</td>\n",
       "      <td>-0.065833</td>\n",
       "      <td>-0.230980</td>\n",
       "      <td>-0.271742</td>\n",
       "      <td>-0.074918</td>\n",
       "      <td>-0.228812</td>\n",
       "      <td>0.200427</td>\n",
       "      <td>0.355319</td>\n",
       "      <td>-0.290702</td>\n",
       "      <td>0.254139</td>\n",
       "      <td>0.350473</td>\n",
       "      <td>0.202767</td>\n",
       "      <td>-0.050586</td>\n",
       "      <td>-0.002205</td>\n",
       "      <td>0.436756</td>\n",
       "      <td>0.288826</td>\n",
       "      <td>-0.283149</td>\n",
       "      <td>-0.032611</td>\n",
       "      <td>-0.313930</td>\n",
       "      <td>-0.305459</td>\n",
       "      <td>0.278596</td>\n",
       "      <td>-0.235510</td>\n",
       "      <td>-0.462689</td>\n",
       "      <td>0.287846</td>\n",
       "      <td>0.197053</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>-0.147399</td>\n",
       "      <td>-0.338517</td>\n",
       "      <td>0.277038</td>\n",
       "      <td>-0.069104</td>\n",
       "      <td>0.260537</td>\n",
       "      <td>0.222542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       407       408       409\n",
       "0  0.168456  0.250936  0.103543  ...  0.061026  0.078947  0.155917\n",
       "1 -0.220249 -0.188598 -0.345878  ...  0.152203 -0.138082 -0.218087\n",
       "2 -0.221206 -0.199571 -0.370084  ...  0.150654 -0.132315 -0.233182\n",
       "3  0.242197  0.227901  0.124662  ... -0.174330  0.278329  0.182786\n",
       "4 -0.350860  0.209580  0.249544  ... -0.069104  0.260537  0.222542\n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "g5yN5OoMcGLl",
    "outputId": "651b02d2-1907-40ac-8ce4-04a13c728f92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  <unk>\n",
       "1    <s>\n",
       "2   </s>\n",
       "3      .\n",
       "4      ,"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(itos)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSKbgwagcJ3C"
   },
   "outputs": [],
   "source": [
    "\n",
    "df2.to_csv('embeddings_transformer_metadata.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "5JxFJdwdcMi2",
    "outputId": "c75243bc-06e5-473e-faa5-5ae06f1998a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2202, -0.1886, -0.3459,  0.1124,  0.0238,  0.1011, -0.0331, -0.1440,\n",
       "         0.4231,  0.0065,  0.0575, -0.1798,  0.0377, -0.1854,  0.2506,  0.0826,\n",
       "         0.1499,  0.0415,  0.2878,  0.1459, -0.3982, -0.0767,  0.0049,  0.4330,\n",
       "         0.5107, -0.1461, -0.0987,  0.0749,  0.2776,  0.2154,  0.2973, -0.5393,\n",
       "         0.0320, -0.1569, -0.2962, -0.0440, -0.0933, -0.0023, -0.0837, -0.1327,\n",
       "         0.2630, -0.1078, -0.0708, -0.2659,  0.0255,  0.2543, -0.2125,  0.1126,\n",
       "         0.0870,  0.1243, -0.2405,  0.0682, -0.1645, -0.1054, -0.2425,  0.1273,\n",
       "         0.1253,  0.0968,  0.0073,  0.2726,  0.4936,  0.2044,  0.2313,  0.3345,\n",
       "         0.1602, -0.0084,  0.0213,  0.1490, -0.1309,  0.0580, -0.0496,  0.3038,\n",
       "        -0.1837, -0.0866,  0.3255, -0.1287,  0.2909, -0.1200, -0.1072, -0.3180,\n",
       "        -0.2481, -0.2994,  0.0709,  0.1636, -0.3588, -0.0551,  0.1584, -0.1144,\n",
       "        -0.0661, -0.0121, -0.0988,  0.1080,  0.0884, -0.0399,  0.2740,  0.0260,\n",
       "         0.4446, -0.1439, -0.1277, -0.2029,  0.2065,  0.0485, -0.2626, -0.0981,\n",
       "        -0.0604,  0.1054, -0.0348,  0.3148,  0.1755, -0.1252, -0.0890, -0.1598,\n",
       "         0.0501, -0.2093,  0.3049,  0.2028, -0.4852,  0.1101,  0.2994,  0.0917,\n",
       "         0.1052, -0.2258, -0.0584,  0.0439, -0.2646, -0.1952,  0.0239, -0.1993,\n",
       "        -0.3674, -0.0343, -0.2421, -0.1667, -0.0497, -0.2913, -0.2172,  0.1313,\n",
       "         0.0663,  0.0898, -0.0827, -0.0566,  0.0020, -0.0154, -0.2592,  0.1973,\n",
       "        -0.0079,  0.2128, -0.5962, -0.1508, -0.0607,  0.1220, -0.1792,  0.1995,\n",
       "        -0.3520,  0.0757,  0.0652, -0.3526,  0.2048,  0.0621,  0.1721, -0.1130,\n",
       "        -0.2007,  0.3504, -0.0621, -0.2884,  0.0954, -0.3039, -0.2067,  0.2590,\n",
       "         0.2779, -0.0124, -0.4596,  0.1060, -0.0214, -0.2112, -0.2159, -0.0167,\n",
       "        -0.2853,  0.0902, -0.0669,  0.2583,  0.0609, -0.0550, -0.1187, -0.3088,\n",
       "        -0.2191,  0.3084, -0.0918, -0.3645, -0.2140, -0.0890,  0.1263, -0.2261,\n",
       "         0.0214,  0.1310, -0.1579, -0.0442, -0.0276,  0.0566, -0.1040,  0.1113,\n",
       "        -0.1121,  0.4092, -0.2985, -0.4422,  0.0997,  0.1892,  0.0511, -0.1039,\n",
       "         0.4390,  0.2450, -0.0206, -0.1765,  0.0278,  0.5254, -0.0158, -0.1377,\n",
       "        -0.1845, -0.1768, -0.2186, -0.2542,  0.0207, -0.2900, -0.1420,  0.0339,\n",
       "        -0.0556, -0.1533, -0.0176, -0.2175, -0.3406, -0.1895,  0.0507,  0.4072,\n",
       "        -0.1962, -0.3094,  0.3710, -0.3235, -0.0561, -0.4511, -0.1537, -0.2298,\n",
       "         0.3334, -0.1913,  0.0023,  0.3425,  0.2943,  0.1227, -0.1058, -0.0854,\n",
       "         0.2880,  0.2984,  0.0721,  0.0491,  0.3039,  0.2709, -0.0575,  0.3290,\n",
       "         0.1491, -0.1797, -0.3234,  0.0424,  0.2266,  0.3917, -0.5896, -0.0577,\n",
       "         0.0659,  0.0779,  0.0051,  0.4180, -0.0251,  0.4702,  0.3890, -0.1146,\n",
       "        -0.2898, -0.2619, -0.2055, -0.1266,  0.2506, -0.3154,  0.0333, -0.0563,\n",
       "         0.1130, -0.2200, -0.2575,  0.0202, -0.0128, -0.2980,  0.0421, -0.4127,\n",
       "        -0.1101, -0.1915,  0.1475, -0.2435, -0.3135, -0.1098, -0.3925, -0.0332,\n",
       "         0.1237, -0.2716, -0.1407, -0.0700,  0.0261, -0.0355, -0.1455,  0.2710,\n",
       "         0.1668, -0.2129,  0.1965,  0.3360, -0.2341,  0.0875,  0.0307,  0.4980,\n",
       "         0.1162, -0.0492,  0.5545, -0.0122,  0.0992, -0.1633,  0.0695, -0.0138,\n",
       "        -0.0322, -0.1532, -0.0885, -0.0400,  0.2335, -0.3093,  0.0293, -0.4859,\n",
       "        -0.0044,  0.2104,  0.0512, -0.2785, -0.2534, -0.0073, -0.2519,  0.3947,\n",
       "        -0.3478,  0.2611, -0.3470,  0.0838, -0.1698,  0.0500,  0.0329,  0.3750,\n",
       "         0.2982,  0.0142, -0.0374, -0.1774,  0.1111, -0.2690,  0.0552,  0.3230,\n",
       "         0.1150, -0.1564,  0.1686, -0.2301, -0.0247,  0.0516,  0.2171,  0.1122,\n",
       "        -0.0195, -0.0464, -0.0920, -0.0278, -0.1797, -0.1997, -0.1284,  0.3314,\n",
       "        -0.1518, -0.0287,  0.2826, -0.1660, -0.1586,  0.3005,  0.1123,  0.2168,\n",
       "         0.2056, -0.1490, -0.0386,  0.1288,  0.2897,  0.5400,  0.0572,  0.1164,\n",
       "        -0.2785,  0.0289,  0.1605,  0.3696, -0.3408, -0.3064, -0.1296,  0.0739,\n",
       "         0.0786, -0.2343, -0.1241, -0.2633,  0.1024,  0.1142, -0.3867,  0.2192,\n",
       "         0.1514, -0.0822,  0.0329,  0.1370,  0.0574,  0.3966, -0.0548,  0.1522,\n",
       "        -0.1381, -0.2181])"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()['encoder.weight'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2NOT1LYcOVU"
   },
   "outputs": [],
   "source": [
    "!cp -r 'embeddings.tsv' 'drive/My Drive/nlp-telugu/transformer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esbMVK07cS9V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "telugu-tansformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
