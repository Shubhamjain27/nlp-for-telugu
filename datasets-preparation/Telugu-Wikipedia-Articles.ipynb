{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drive/My Drive/nlp-telugu/inltk/dataset/telugu_wiki_links.csv')\n",
    "unique_links = df['link'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_buffer(response):\n",
    "    response.text = response.read()\n",
    "    return response.text.decode('utf-8')\n",
    "\n",
    "    \n",
    "def get_data_from_url(url):\n",
    "    try:\n",
    "        r = urlopen(url)\n",
    "        doc = readout_buffer(r)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        doc = \"\"\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selectolax\n",
    "from selectolax.parser import HTMLParser\n",
    "def get_details(url):\n",
    "    doc = get_data_from_url(url)\n",
    "    try: \n",
    "        html_doc = HTMLParser(doc)\n",
    "        t = '\\n '.join(n.text() for n in html_doc.css(\"title\"))\n",
    "        a = '\\n '.join(n.text() for n in html_doc.css(\"p\"))\n",
    "    except:\n",
    "        t = \"\"\n",
    "        a = \"\"\n",
    "    return t, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import multiprocessing.dummy as mpd\n",
    "import time\n",
    "\n",
    "start = datetime.now()\n",
    "cpu_cores = mp.cpu_count()\n",
    "print('parallelising the task on {} cpu cores'.format(cpu_cores))\n",
    "\n",
    "rows = []\n",
    "count = 0\n",
    "# divide pool\n",
    "pool = mpd.Pool(processes=cpu_cores)\n",
    "# iter over \n",
    "for row in pool.imap(get_details, unique_links):\n",
    "    rows.append(row)    \n",
    "    count = count + 1\n",
    "    # print/save\n",
    "    if not count%100:\n",
    "        print('done for,', count)\n",
    "    if not count%10000:\n",
    "        df = pd.DataFrame(rows, columns = ['title', 'text'])\n",
    "        df.to_parquet('telugu_wikipedia_dataset.parquet', index = None)\n",
    "        print(\"Done for {} rows ---> {}\".format(count, datetime.now() - start))\n",
    "# close the pool\n",
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
